{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this part, for now\n",
    "# !virtualenv SubtextScribe # create virtual environment\n",
    "# activate virtual environment\n",
    "# %cd SubtextScribe\n",
    "# %source bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure pip is up to date\n",
    "# %pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure necessary libraries are installed\n",
    "!pip list # run this to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not, install libraries here\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install transformers\n",
    "%pip install numpy pandas matplotlib spacy sklearn nltk praw\n",
    "%pip install virtualenv # install virtualenv package\n",
    "%pip install transformers torch accelerate # install Hugging Face 'transformers' library, PyTorch, and 'accelerate' for efficient model training / parallelization\n",
    "%pip install ipywidgets --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm # download small English model (essential for performing various NLP tasks on English text through tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "# for model training\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for model evaluation\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone LitBank git repository into designated directory\n",
    "# !cd ** your_directory_here **\n",
    "# !git clone https://github.com/dbamman/litbank.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file paths\n",
    "base_path = \"your path here\" # make sure this reflects your own path (change it if necessary)\n",
    "entities_path = os.path.join(base_path, \"entities\", \"brat\")\n",
    "events_path = os.path.join(base_path, \"events\", \"brat\")\n",
    "original_texts_path = os.path.join(base_path, \"original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read original texts\n",
    "def read_original_texts(path):\n",
    "    texts = {}\n",
    "    for filename in os.listdir(path):\n",
    "        with open(os.path.join(path, filename), 'r') as file:\n",
    "            texts[filename] = file.read()\n",
    "    return texts\n",
    "\n",
    "original_texts = read_original_texts(original_texts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_files(path):\n",
    "    texts = {}\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(path, filename), 'r', encoding='utf-8') as file:\n",
    "                base_filename = filename.replace(\"_brat.txt\", \"\")\n",
    "                texts[base_filename] = file.read()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ann_files(path):\n",
    "    annotations = {}\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith('.ann'):\n",
    "            with open(os.path.join(path, filename), 'r', encoding='utf-8') as file:\n",
    "                base_filename = filename.replace(\"_brat.ann\", \"\")\n",
    "                annotations[base_filename] = [line.strip() for line in file.readlines()]\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = process_text_files(os.path.join(base_path, \"entities\", \"brat\")) # assuming the text files are the same in both entities and events folders\n",
    "entities_annotations = process_ann_files(os.path.join(base_path, \"entities\", \"brat\"))\n",
    "events_annotations = process_ann_files(os.path.join(base_path, \"events\", \"brat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging (print texts)\n",
    "print(\"\\Texts:\")\n",
    "for filename, text in texts.items():\n",
    "    print(f\"\\nFile: {filename}\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging (print entities annotations)\n",
    "print(\"Entities Annotations:\")\n",
    "for filename, annotation in entities_annotations.items():\n",
    "    print(f\"\\nFile: {filename}\")\n",
    "    for ann in annotation:\n",
    "        print(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging (print events annotations)\n",
    "print(\"Events Annotations:\")\n",
    "for filename, annotation in events_annotations.items():\n",
    "    print(f\"\\nFile: {filename}\")\n",
    "    for ann in annotation:\n",
    "        print(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entities_annotations.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # load small English model from Spacy for nlp tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    processed_text = \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse .ann files and align them with text\n",
    "def parse_annotations(ann_lines):\n",
    "    annotations = []\n",
    "    for line in ann_lines:\n",
    "        if line:\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) > 2:\n",
    "                ann_id, ann_info, ann_text = parts\n",
    "                ann_info_parts = ann_info.split(' ')\n",
    "                if len(ann_info_parts) == 3:\n",
    "                    ann_type, start, end = ann_info_parts\n",
    "                    annotations.append({'id': ann_id, 'type': ann_type, 'start': int(start), 'end': int(end), 'text': ann_text})\n",
    "                # else:\n",
    "                    # print(f\"Unexpected format in annotation: {line}\") # debugging\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = {key: preprocess_text(text) for key, text in texts.items()}\n",
    " # entities and events annotations are both dictionaries where each value is a list of lines\n",
    "parsed_entities = {key: parse_annotations(ann_lines) for key, ann_lines in entities_annotations.items()}\n",
    "parsed_events = {key: parse_annotations(ann_lines) for key, ann_lines in events_annotations.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate annotations into preprocessed texts\n",
    "def integrate_annotations(text, annotations):\n",
    "    for ann in sorted(annotations, key=lambda x: x['start'], reverse=True):\n",
    "        text = text[:ann['start']] + f\" <{ann['type']}>\" + ann['text'] + f\"</{ann['type']}> \" + text[ann['end']:]\n",
    "    return text\n",
    "\n",
    "integrated_texts = {key: integrate_annotations(processed_texts[key], parsed_entities[key] + parsed_events[key]) for key in processed_texts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pre-Tune Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load pre-trained GPT-2 model and its tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# load pre-trained GPT-2 Large model and its tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
    "\n",
    "# # load pre-trained GPT-2 XL model and its tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "# model = GPT2LMHeadModel.from_pretrained('gpt2-xl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of prompts to use for perplexity evaluation\n",
    "prompts = [\n",
    "    \"[Topic: Space Exploration] As the spaceship neared the mysterious planet, Captain Lyra noticed an unusual signal coming from the surface...\",\n",
    "    \"[Topic: Medieval Kingdom] Deep within the ancient castle's walls, Sir Gareth stumbled upon a secret passage that had been hidden for centuries...\",\n",
    "    \"[Topic: Underwater Adventure] In the depths of the uncharted ocean, marine biologist Dr. Elara spotted a strange glow emanating from an underwater cave...\",\n",
    "    \"[Topic: Time Travel] When Professor Milton activated the time machine, he didn't expect to find himself in the middle of a bustling Victorian market...\",\n",
    "    \"[Topic: Lost Civilization] Amidst the dense jungle, explorer Isabella uncovered the ruins of a civilization that maps had never documented...\",\n",
    "    \"[Topic: Futuristic City] In the year 2150, Detective Kai roamed the neon-lit streets of Neo-Tokyo, following the trail of a mysterious technology heist...\",\n",
    "    \"[Topic: Magical School] On her first day at the Arcane Academy, young witch Elowen discovered a magical artifact that had been hidden in the library's oldest section...\",\n",
    "    \"[Topic: Dystopian World] In a world where the sun never rose, Luna and her rebel companions planned their next move against the oppressive regime...\",\n",
    "    \"[Topic: Alien Encounter] As the alien spacecraft landed in the quiet countryside, farmer Jim cautiously approached, unaware of how this encounter would change his life...\",\n",
    "    \"[Topic: Arctic Expedition] Trapped in a fierce blizzard during their Arctic expedition, Dr. Hansen and her team found refuge in an ice cave with mysterious carvings...\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for generating a response based on given prompt\n",
    "def generate_text(model, tokenizer, prompt, max_length=150):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
    "    outputs = model.generate(\n",
    "        inputs,                         # input token IDs to model\n",
    "        attention_mask=attention_mask,  # mask that indicates which tokens to pay attention to and which to ignore\n",
    "        max_length=max_length,          # maximum length of the sequence to be generated\n",
    "        num_return_sequences=1,         # number of different sequences to generate from the same prompt (a num greater than 1 allows model to generate multiple different continuations from same prompt)\n",
    "        do_sample=True,                 # enables / disables sampling (if set to True, the model samples from probability distribution of next token, leading to more varied and random outputs, and if False, the model deterministically picks next most likely token)\n",
    "        temperature=0.8,                # controls randomness of output (a value of 1.0 means no change to original probabilities, values less than 1.0 make model outputs more deterministic (less random), while values greater than 1.0 introduce more randomness)\n",
    "        top_k=30,                       # limits number of highest probability vocab tokens considered for each step (a lower top_k leads to more deterministic outputs, while a higher top_k allows for more varied outputs)\n",
    "        top_p=0.92,                     # considers smallest set of tokens whose cumulative probability exceeds threshold top_p (dynamically adapts size of token set based on next token's probability distribution)\n",
    "        repetition_penalty=1.3          # penalizes model for repeating same token (a value greater than 1.0 discourages repetition, while a value less than 1.0 encourages it)\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation (qualitative evaluation)\n",
    "custom_prompt = \"Trapped in a fierce blizzard during their expedition, the scientists found refuge in an ice cavee\" # you can prompt model with some starting text and generate a continuation\n",
    "\n",
    "# generate text with model\n",
    "generated_text = generate_text(model, tokenizer, custom_prompt)\n",
    "\n",
    "# text formatting (insert newlines after certain number of words)\n",
    "def insert_newlines(text, word_count=20):\n",
    "    words = text.split()\n",
    "    lines = [' '.join(words[i:i+word_count]) for i in range(0, len(words), word_count)]\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "formatted_text = insert_newlines(generated_text, word_count=20)\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metric functions to be used in evaluation stage\n",
    "\n",
    "# perplexity function\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    outputs = model(inputs, labels=inputs)\n",
    "    loss = outputs.loss\n",
    "    return torch.exp(loss).item()\n",
    "\n",
    "# Self-BLEU score function\n",
    "def calculate_self_bleu(texts):\n",
    "    scores = []\n",
    "    for i, candidate in enumerate(texts):\n",
    "        references = texts[:i] + texts[i+1:]\n",
    "        scores.append(sentence_bleu(references, candidate, smoothing_function=SmoothingFunction().method1))\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation function (pre / post-tuning)\n",
    "def evaluate_model(model, tokenizer, prompts, num_samples=10):\n",
    "    total_perplexity = 0\n",
    "    generated_responses = []\n",
    "\n",
    "    # generate one response for each prompt and calculate perplexity\n",
    "    for i in range(num_samples):\n",
    "        model_input_prompt = prompts[i]\n",
    "        generated_text = generate_text(model, tokenizer, model_input_prompt)\n",
    "        generated_responses.append(generated_text)\n",
    "\n",
    "        perplexity = calculate_perplexity(model, tokenizer, model_input_prompt)\n",
    "        total_perplexity += perplexity\n",
    "\n",
    "    # calculate Self-BLEU using list of generated responses\n",
    "    self_bleu = calculate_self_bleu(generated_responses)\n",
    "    average_perplexity = total_perplexity / num_samples\n",
    "\n",
    "    return average_perplexity, self_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert texts to a list\n",
    "integrated_text_list = list(integrated_texts.values())\n",
    "\n",
    "# pre fine-tuned perplexity and BLEU score metrics (quantitative evaluation)\n",
    "pre_perplexity, pre_self_bleu = evaluate_model(model, tokenizer, prompts, num_samples=10)\n",
    "print(f\"Average Perplexity (Pre-Tuning): {pre_perplexity}, Self-BLEU Score (Pre-Tuning): {pre_self_bleu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set padding token to be the same as EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization and dataset preparation for GPT-2\n",
    "class GPT2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length=1024):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "            self.labels.append(torch.tensor(encodings_dict['input_ids'])) # for language modeling, the labels are the input IDs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attn_masks[idx],\n",
    "            'labels': self.labels[idx]  # ensure labels are included here\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare and split datasets\n",
    "integrated_text_list = list(integrated_texts.values())  # convert dictionary values to a list\n",
    "train_texts, val_texts = train_test_split(integrated_text_list, test_size=0.1) # # split the encoded texts into training / validation sets (10% used for validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GPT2Dataset objects for training / validation sets\n",
    "train_dataset = GPT2Dataset(train_texts, tokenizer)\n",
    "val_dataset = GPT2Dataset(val_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up training arguments (where parameters like batch size, number of epochs, learning rate, etc. are defined)\n",
    "\n",
    "# # 1: less intensive version\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=2,  # further reduced batch size\n",
    "#     gradient_accumulation_steps=4,  # increased gradient accumulation\n",
    "#     warmup_steps=500,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir='./logs',\n",
    "# )\n",
    "\n",
    "# # 2: original training version\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',          # output directory\n",
    "#     num_train_epochs=3,              # total number of training epochs\n",
    "#     per_device_train_batch_size=4,   # batch size per device during training\n",
    "#     warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#     weight_decay=0.01,               # strength of weight decay\n",
    "#     logging_dir='./logs',            # directory for storing logs\n",
    "# )\n",
    "\n",
    "# # 3: more intensive version\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     num_train_epochs=5,              # increased number of epochs\n",
    "#     per_device_train_batch_size=8,   # increased batch size\n",
    "#     warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#     weight_decay=0.01,               # strength of weight decay\n",
    "#     logging_dir='./logs',            # directory for storing logs\n",
    "#     learning_rate=5e-5,              # learning rate\n",
    "# )\n",
    "\n",
    "# 4: even more intensive version\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,             # further increased number of epochs\n",
    "    per_device_train_batch_size=16,  # further increased batch size\n",
    "    warmup_steps=1000,               # adjusted number of warmup steps\n",
    "    weight_decay=0.02,               # increased weight decay\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=3e-5,              # adjusted learning rate\n",
    "    lr_scheduler_type='cosine',      # advanced learning rate scheduler\n",
    "    evaluation_strategy='steps',     # evaluate more frequently\n",
    "    eval_steps=500,                  # evaluation step\n",
    "    gradient_accumulation_steps=1,   # adjust based on memory\n",
    "    max_grad_norm=1.0,               # gradient clipping\n",
    "    fp16=True if torch.cuda.is_available() else False, # enables mixed precision training if supported\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize trainer with model, training arguments, and training dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment variable to disable upper limit for memory allocations (can potentially lead to system instability, so use cautiously)\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect() # garbage collection\n",
    "torch.cuda.empty_cache() # cache clearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # train model\n",
    "model.save_pretrained('./fine_tuned_gpt2') # save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Post-Tune Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fine-tuned model\n",
    "model = GPT2LMHeadModel.from_pretrained('./fine_tuned_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model for evaluation\n",
    "model.eval() # putting model in eval mode disables dropout layers and batch normalization during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation (qualitative evaluation)\n",
    "custom_prompt = \"your input prompt here\" # you can prompt model with some starting text and generate a continuation\n",
    "\n",
    "# generate text with model\n",
    "generated_text = generate_text(model, tokenizer, custom_prompt)\n",
    "\n",
    "# text formatting (insert newlines after certain number of words)\n",
    "def insert_newlines(text, word_count=20):\n",
    "    words = text.split()\n",
    "    lines = [' '.join(words[i:i+word_count]) for i in range(0, len(words), word_count)]\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "formatted_text = insert_newlines(generated_text, word_count=20)\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post fine-tuned perplexity and BLEU score metrics (quantitative evaluation)\n",
    "post_perplexity, post_self_bleu = evaluate_model(model, tokenizer, prompts, num_samples=10)\n",
    "print(f\"Average Perplexity (Post-Tuning): {post_perplexity}, Self-BLEU Score (Post-Tuning): {post_self_bleu}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
